{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd4dca1",
   "metadata": {},
   "source": [
    "# TD Régression \n",
    "Le jeu de données utilisé est issu du livre d'Aurélien Géron, \"Machine Learning avec Scikit Learn\" (Dunod edition) ; c'est une version modifiée d'un jeu de données original de Luís Torgo. Ces données ont été proposées dans un article de 1997 du journal Statistics and Probability Letters. Ce sont des données du recensement de Californie de 1990. Chaque ligne correspond à un groupe de \"blocks\", ou district, (plus petite unité géographique utilisée par le Bureau du recensement aux États-Unis, représentatif d'une population entre 600 et 3 000 personnes). A ces données de recensement, un attribut supplémentaire sur la proximité avec l'Océan Pacifique a été ajouté : ocean_proximity.\n",
    "\n",
    "### Objectifs\n",
    "Les principaux objectifs de la séances sont de comprendre les étapes de création d'un modèle de régression et de s'approprier le code Python correspondant : \n",
    "- 1. Importation des données, \n",
    "- 2. visualisation et analyse des données, \n",
    "- 3. construction d'un premier modèle (quick & dirty) : choix et entrainement \n",
    "- 5. évaluation des performances du modèle, \n",
    "- 6. amélioration du modèle (travail sur les données; choix des algorithmes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b14e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation bibiliothèques\n",
    "import pandas as pd\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib as mlp\n",
    "import scikit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e36a0cf",
   "metadata": {},
   "source": [
    "# 1. Importation des données\n",
    "Vous devez :\n",
    "- vérifier que les données sont bien importées \n",
    "- comprendre à quoi elles correspondent \n",
    "- évaluer la volumétrie\n",
    "- comprendre en quoi consistera le problème de régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95050d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           20640 non-null  float64\n",
      " 1   latitude            20640 non-null  float64\n",
      " 2   housing_median_age  20640 non-null  float64\n",
      " 3   total_rooms         20640 non-null  float64\n",
      " 4   total_bedrooms      20433 non-null  float64\n",
      " 5   population          20640 non-null  float64\n",
      " 6   households          20640 non-null  float64\n",
      " 7   median_income       20640 non-null  float64\n",
      " 8   median_house_value  20640 non-null  float64\n",
      " 9   ocean_proximity     20640 non-null  object \n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     9136\n",
       "INLAND        6551\n",
       "NEAR OCEAN    2658\n",
       "NEAR BAY      2290\n",
       "ISLAND           5\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chargement des données à partir du csv\n",
    "housing = pd.read_csv(\"housing2.csv\")\n",
    "\n",
    "# affichage des premières lignes des données\n",
    "housing.head()\n",
    "\n",
    "# résumé de la volumétrie et du type des données\n",
    "housing.info()\n",
    "\n",
    "# résumé statistique\n",
    "housing.describe() \n",
    "\n",
    "# Les valeurs de \"ocean_proximity\" ne sont pas affichées ! \n",
    "housing[\"ocean_proximity\"].unique()\n",
    "\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee662e",
   "metadata": {},
   "source": [
    "# 2. Visualisation, Nettoyage et Analyse des données\n",
    "## 2.1 Visualisation\n",
    "A partir des visualisations proposées (vous pouez en faire d'autres), quelles sont vos premières conclusions sur les données utilisées ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de chaque variable sous forme d'un histogramme\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50,figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6249c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation \"géographique\"\n",
    "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6df8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation \"géographique\" en couleur\n",
    "housing.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\",alpha=0.4,\n",
    "            s=housing[\"population\"]/100,label=\"population\",\n",
    "            c=\"median_house_value\",cmap=plt.get_cmap(\"jet\"),\n",
    "            colorbar=True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86a359",
   "metadata": {},
   "source": [
    "## 2.2 Nettoyage - gestion des valeurs manquantes (ici nulles)\n",
    "Avez vous identifiés que des valeurs sont manquantes ? \n",
    "Si oui elles peuvent être traitées de la manière suivante :\n",
    "- on supprime les valeurs(lignes) correspondantes\n",
    "- on se débarrasse de la variable/feature(colonne) complétement\n",
    "- on remplace les valeurs manquantes avec une certaine valeur (zéro, moyenne, médiane, etc.), c'est l'imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ef7678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitude               0\n",
      "latitude                0\n",
      "housing_median_age      0\n",
      "total_rooms             0\n",
      "total_bedrooms        207\n",
      "population              0\n",
      "households              0\n",
      "median_income           0\n",
      "median_house_value      0\n",
      "ocean_proximity         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# On peut tester s'il y a des valeurs non définies\n",
    "print(housing.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a688cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Imputation par valeur médiane\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[1;32m      3\u001b[0m imputer\u001b[38;5;241m=\u001b[39mSimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Attention : commme SimpleImputer ne fonctionne que sur des variables/features numériques, \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# on doit retirer celles qui ne le sont pas pour Imputer\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Récupération des variables numériques\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Imputation par valeur médiane\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Attention : commme SimpleImputer ne fonctionne que sur des variables/features numériques, \n",
    "# on doit retirer celles qui ne le sont pas pour Imputer\n",
    "\n",
    "# Récupération des variables numériques\n",
    "housing_num=housing.drop(\"ocean_proximity\",axis=1)\n",
    "\n",
    "# Imputation\n",
    "imputer.fit(housing_num) \n",
    "housing_num_tr=imputer.transform(housing_num)\n",
    "\n",
    "# transformation en dataframe : housing_num_sans0\n",
    "housing_num_sans0=pd.DataFrame(housing_num_tr,columns=housing_num.columns)\n",
    "\n",
    "print(housing_num_sans0.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a95c13",
   "metadata": {},
   "source": [
    "## 2.3 Analyse des données  - étude des corrélations\n",
    "Comment les corrélations vont vous aider à construire votre premier modèle de régression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7423a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la matrice des coefficients de Pearson \n",
    "corr_matrix=housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice des coefficients de Pearson\n",
    "import seaborn as sns\n",
    "heat_map = sns.heatmap(corr_matrix, center=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd20eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la scatter matrice \n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(housing,figsize=(12,8))\n",
    "\n",
    "# possibilité de le faire sur certaines variables. par exemple:\n",
    "#attributs=[\"median_house_value\",\"median_income\",\"total_rooms\",\"housing_median_age\"]\n",
    "#scatter_matrix(housing[attributs],figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa52f8",
   "metadata": {},
   "source": [
    "Quelles sont vos conclusions ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc9f8d9",
   "metadata": {},
   "source": [
    "# 3. Construction d'un premier modèle (quick & dirty),\n",
    "\n",
    "## 3.1 Choix des entrées/features et de la sortie/valeur à prédire\n",
    "Vous disposez des données numériques nettoyées dans housing_num_sans0\n",
    "\n",
    "Mais la variable catégorielle (proximité de l'océan est encore sous forme de chaines de caractères. Il est nécessaire de la coder sous forme numérique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960fd220",
   "metadata": {},
   "source": [
    "## gestion des valeurs non  numériques (optionel)\n",
    "La variable ocean_proximity n'est pas numérique, il est donc nécessaire de la coder sous forme numérique. Deux méthodes sont proposées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da60cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode 1\n",
    "# Chaque classe est codée en un entier compris entre 0 et N-1 (N: nombre de classes)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder1=LabelEncoder()\n",
    "housing_cat=housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded=encoder1.fit_transform(housing_cat)\n",
    "housing_cat_encoded\n",
    "\n",
    "# remarque l'opération inverse est alors\n",
    "# encoder1.inverse_transform(housing_cat_encoded)\n",
    "\n",
    "# Quel est le défaut d'un tel codage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80ce185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode 2\n",
    "# Chaque classe est codée par une liste avec N-1 zéros et 1 un. La position du un indique la classe\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder2=OneHotEncoder()\n",
    "housing_cat_hot= encoder2.fit_transform(housing[[\"ocean_proximity\"]])\n",
    "# autre syntaxe:housing_cat_hot=encoder2.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "\n",
    "# On obtient un objet de type matrice creuse par défaut, besoin de le convertir en matrice\n",
    "housing_cat_hot_encoded=housing_cat_hot.toarray()\n",
    "housing_cat_hot_encoded\n",
    "\n",
    "# Quel est le défaut d'un tel codage?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37b7a00",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03a34b6e",
   "metadata": {},
   "source": [
    "## Choix des entrées/sorties\n",
    "\n",
    "Vous avez à choisir les données en entrée et sortie de votre premier modèle qui servira de référence pour la suite : \n",
    "- Entrées: X\n",
    "- Sortie: y\n",
    "\n",
    "Conseil : démarrez avec un premier modèle simple que vous chercherez à améliorer par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae274b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des données nettoyées (au cas où on modifie housing_num_sans0)\n",
    "hoosing_save = pd.DataFrame.copy(housing_num_sans0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116aad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de X : par exemple copie puis suppression des colonnes hors modèle\n",
    "X = pd.DataFrame.copy(housing_num_sans0)\n",
    "X = pd.DataFrame.drop(X,columns=[\"housing_median_age\",\"total_rooms\",\"total_bedrooms\",\"population\",\"households\", \"median_house_value\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8b580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de y\n",
    "y = housing_num_sans0[[\"median_house_value\"]]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5296e",
   "metadata": {},
   "source": [
    "# 3.2 Normalisation/Standardisation des données\n",
    "L'ordre de grandeur des valeurs numériques va avoir un effet sur le processus d'entrainement. Il est nécessaire de normaliser ou standardiser les données du modèle.\n",
    "\n",
    "Standardisation d'une variable avec StandardScaler: \n",
    "- nouvelle variable a moyenne nulle et écart type égal à 1 \n",
    "\n",
    "Normalisation d'une variable: \n",
    "- nouvelle variable comprise entre 0 et 1 : avec MinMaxScaler\n",
    "- nouvelle variable comprise entre -1 et +1 : avec MaxAbsScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd031ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_stand = scaler.transform(X)\n",
    "scaler.fit(y)\n",
    "y_stand = scaler.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4e279",
   "metadata": {},
   "source": [
    "## 3.3 Choix d'un type de modèle et des données d'entrainement (et de test)\n",
    "On ne va utiliser que 80% (par exemple) des données pour l'entrainement du modèle. Les 20% restant seront utilisés plus tard... On appèlera \"données d'entraintement\" les 80% et \"données de test\" les 20%.\n",
    "On va tester comme premier type de modèle, un modèle de régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9debb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des données d'entrainement (X_train / y_train) et de test (X_test / y_test )\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_stand, y_stand, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a2da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix d'un modèle de regression linéaire pour l'entrainement\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg=LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bc56e6",
   "metadata": {},
   "source": [
    "## 3.4 Entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ba021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainement\n",
    "lin_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97bafb",
   "metadata": {},
   "source": [
    "# 4. Evaluation des performances du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "Prediction_train=lin_reg.predict(X_train)\n",
    "\n",
    "# Choix d'une métrique : par exemple RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_train = mean_squared_error(y_train,Prediction_train)\n",
    "RMSE_train = np.sqrt(MSE_train)\n",
    "RMSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données de test\n",
    "Prediction_test=lin_reg.predict(X_test)\n",
    "\n",
    "# Choix d'une métrique : par exemple RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "MSE_test = mean_squared_error(y_test,Prediction_test)\n",
    "RMSE_test = np.sqrt(MSE_test)\n",
    "RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f46af1",
   "metadata": {},
   "source": [
    "Quelles sont vos conclusions sur ce premier modèle ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3e4a4",
   "metadata": {},
   "source": [
    "# 6. Amélioration des performances du modèle\n",
    "Il existe de très nombreuses manières d'obtenir un modèle plus performant. Tout d'abord en examinant en détails  les situations où le modèle est médiocre afin d'en déduire des solutions d'amélioration.\n",
    "Sinon, de manière systématique, voici des pistes d'amélioration :\n",
    "- ajouter ou retirer des entrées/features\n",
    "- créer de nouveaux features; par exemple, le nombre total de chambres dans un district n'a peut être pas beaucoup de signification mais le nombre moyen de chambre par foyer sans doute plus etc.\n",
    "- régulariser le modèle linéaire : regression ridge, regression lasso\n",
    "- changer de type de modèles, par exemple on considérant des arbres de regression ou des forêts aléatoires\n",
    "- changer de stratégie d'entrainement avec la validation croisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19a72eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quelques fonctions\n",
    "\n",
    "# regression ridge\n",
    "# from sklearn.linear_model import Ridge\n",
    "# lin_reg_ridge = Ridge(alpha=1.0)\n",
    "# lin_reg_ridge.fit(..., ...)\n",
    "# ...=lin_reg_ridge.predict(...)\n",
    "\n",
    "# regression lasso\n",
    "# lin_reg_lasso = linear_model.Lasso(alpha=0.1)\n",
    "# lin_reg_lasso.fit(..., ...)\n",
    "# ...=lin_reg_lasso.predict(...)\n",
    "\n",
    "# arbres de regression\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# tree_reg=DecisionTreeRegressor()\n",
    "# tree_reg.fit(...,...)\n",
    "# ...=tree_reg.predict(...)\n",
    "# tree_mse= mean_squared_error(...,...)\n",
    "# tree_rmse = np.sqrt(tree_mse)\n",
    "\n",
    "# Validation croisée (ici en exemple avec arbre de regression)\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# scores=cross_val_score(tree_reg,...,...,scoring=\"neg_mean_squared_error\",cv=10)\n",
    "# rmse_scores=np.sqrt(-scores)\n",
    "# remarque : avec cross_val_score, on ne cherche pas à minimiser une fonction cout \n",
    "# mais à maximiser une fonction \"utilité\" d'où le signe négatif.\n",
    "\n",
    "\n",
    "# Forêt aléatoire + validation croisée\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# forest_reg = RandomForestRegressor()\n",
    "# rf_scores=cross_val_score(forest_reg,...,...,scoring=\"neg_mean_squared_error\",cv=5)\n",
    "# rf_rmse_scores=np.sqrt(-rf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f9092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76f220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
